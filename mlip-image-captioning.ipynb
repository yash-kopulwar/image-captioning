{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# kaggle data used:\n",
    "# flickr8k dataset:- https://www.kaggle.com/adityajn105/flickr8k\n",
    "# glove embeddings:- https://www.kaggle.com/incorpes/glove6b200d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model, load_model,Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "img_path=\"Images/\"\n",
    "images = glob(img_path+'*.jpg')\n",
    "len(images)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Images\\\\1009434119_febe49276a.jpg'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractime image features using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 3s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Downloading and building the VGG16 model\n",
    "\n",
    "'''\n",
    "model=VGG16()\n",
    "model.layers.pop()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "features = {}\n",
    "\n",
    "for i in tqdm(images):\n",
    "    img=load_img(i,target_size=(224, 224,3))\n",
    "    img=img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img=preprocess_input(img)\n",
    "    feature = model.predict(img)\n",
    "    feature=feature.reshape((-1,))\n",
    "    \n",
    "    img_id=i.replace(\"../input/flickr8k/Images/\",\"\")\n",
    "    \n",
    "    features[img_id]=feature\n",
    "\n",
    "dump(features, open(\"features.p\",\"wb\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open(\"./features.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4096,), (4096,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features[list(features.keys())[0]]).shape, (features[list(features.keys())[254]]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading caption file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " '1000268201_693b08cb0e.jpg,A girl going into a wooden building .',\n",
       " '1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .',\n",
       " '1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .',\n",
       " '1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .',\n",
       " '1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting',\n",
       " '1001773457_577c3a7d70.jpg,A black dog and a tri-colored dog playing with each other on the road .',\n",
       " '1001773457_577c3a7d70.jpg,A black dog and a white dog with brown spots are staring at each other in the street .',\n",
       " '1001773457_577c3a7d70.jpg,Two dogs of different breeds looking at each other on the road .',\n",
       " '1001773457_577c3a7d70.jpg,Two dogs on pavement moving toward each other .',\n",
       " '1002674143_1b742ab4b8.jpg,A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .',\n",
       " '1002674143_1b742ab4b8.jpg,A little girl is sitting in front of a large painted rainbow .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_txt = open(\"captions.txt\", 'rb').read().decode('utf-8')\n",
    "img_cap_corpus=captions_txt.split('\\n')\n",
    "img_cap_corpus.pop(0)\n",
    "\n",
    "img_cap_corpus[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combining the images data and caption data in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1295669416_21cabf594d.jpg',\n",
       " ['A girl dressed in a red and black top with black pants is sitting on a wall .',\n",
       "  'A girl in a red and black striped shirt sits on a brick wall in front of a tropical plant .',\n",
       "  'A girl wearing a red and black striped shirt is sitting on a brick wall near a flower garden .',\n",
       "  'A lady in a red and black striped shirt is sitting on a retaining wall .',\n",
       "  'A woman in semi-formal attire is sitting on a concrete wall .'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_cap_dic={}\n",
    "for line in img_cap_corpus:   \n",
    "    img_cap = line.split(',')\n",
    "    \n",
    "    if(len(img_cap)>=2):\n",
    "        img_id=img_cap[0]\n",
    "        caption=img_cap[1]\n",
    "\n",
    "        if img_id not in img_cap_dic:\n",
    "            img_cap_dic[img_id] = [caption]\n",
    "        else:\n",
    "            img_cap_dic[img_id].append(caption)\n",
    "\n",
    "            \n",
    "list(img_cap_dic.keys())[254], img_cap_dic[list(img_cap_dic.keys())[254]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning text and adding end points to captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessed(text):\n",
    "    modified = text.lower()    \n",
    "    \n",
    "    tokens = modified.split(' ')\n",
    "    for word in tokens:                   # this part is removing single letter words\n",
    "        if len(word)<2:                   # because, there is no embedding for the letter 'a' in glove200\n",
    "            tokens.remove(word)          \n",
    "    modified=\" \".join(tokens)\n",
    "    \n",
    "    if modified.split(' ')[0]!='start_':\n",
    "        modified = 'start_ ' + modified + ' _end'\n",
    "        \n",
    "    return modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1295669416_21cabf594d.jpg',\n",
       " ['start_ girl dressed in red and black top with black pants is sitting on wall _end',\n",
       "  'start_ girl in red and black striped shirt sits on brick wall in front of tropical plant _end',\n",
       "  'start_ girl wearing red and black striped shirt is sitting on brick wall near flower garden _end',\n",
       "  'start_ lady in red and black striped shirt is sitting on retaining wall _end',\n",
       "  'start_ woman in semi-formal attire is sitting on concrete wall _end'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k, caps in img_cap_dic.items():\n",
    "    for cap in caps:\n",
    "        img_cap_dic[k][caps.index(cap)] = preprocessed(cap)\n",
    "\n",
    "list(img_cap_dic.keys())[254], img_cap_dic[list(img_cap_dic.keys())[254]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maximum length of caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length=0\n",
    "for caps in img_cap_dic.values():\n",
    "    for cap in caps:\n",
    "        if max_length<len(cap.split(\" \")):\n",
    "            max_length=len(cap.split(\" \"))\n",
    "\n",
    "max_length   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing captions and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8091/8091 [00:32<00:00, 246.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<oov>')\n",
    "\n",
    "for k,caps in tqdm(img_cap_dic.items()):\n",
    "    tokenizer.fit_on_texts(caps)\n",
    "    \n",
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8317"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8091/8091 [00:01<00:00, 7094.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# specifying tokens and padding\n",
    "\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "\n",
    "sequence={}\n",
    "for k,caps in tqdm(img_cap_dic.items()):\n",
    "        sequence[k]=pad_sequences(tokenizer.texts_to_sequences(caps), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "        tokenizer.texts_to_sequences(caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 31),\n",
       " '1072153132_53d2bb1b60.jpg',\n",
       " array([[   3,   15,    8,   14,    9,  356,  106,    4,  241,    2,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   3,    9,    8,  140,   40,    2,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   3,   52,    9,    7,   45,   20,  164,  310,  744,   23,  156,\n",
       "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   3,    9,  247,   36,  269,  140,   40,   33,  127,   53,    2,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [   3, 1732,    9,   45,   20,  164,  140,   40,    4,  127,   53,\n",
       "            2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 53\n",
    "(sequence[list(sequence.keys())[q]]).shape, list(sequence.keys())[q], sequence[list(sequence.keys())[q]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.       , 0.9586388, ..., 0.       , 0.       ,\n",
       "       0.       ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, desc_list in img_cap_dic.items():\n",
    "    #retrieve photo features\n",
    "    feature = features[key][0]\n",
    "    \n",
    "features[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  generator = data_generator(img_cap_dic, features, tokenizer, max_length, number_pics_per_bath)\n",
    "# This function will be used while training\n",
    "\n",
    "#'''\n",
    "def data_generator(descriptions, features, tokenizer, max_length,batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    \n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            #retrieve photo features\n",
    "            feature = features[key]            \n",
    "            \n",
    "            # walk through each description for the image\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "                # split one sequence into multiple X,y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            \n",
    "             # yield the batch data\n",
    "            if n==batch:\n",
    "                yield ([np.array(X1), np.array(X2)], np.array(y))\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0\n",
    "            \n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading glove embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {} \n",
    "glove = open('glove.6B.200d.txt', 'r', encoding = 'utf-8').read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400001/400001 [00:20<00:00, 19944.49it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for line in tqdm(glove.split(\"\\n\")): \n",
    "    values = line.split(\" \") \n",
    "    word = values[0] \n",
    "    indices = np.asarray(values[1: ], dtype = 'float32') \n",
    "    embeddings[word] = indices \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8317, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "emb_matrix = np.zeros((vocab_size, embedding_dim)) \n",
    "for word, i in tokenizer.word_index.items(): \n",
    "    embedding_vec = embeddings.get(word) \n",
    "    if embedding_vec is not None: \n",
    "        emb_matrix[i] = embedding_vec \n",
    "        \n",
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32927999,  0.25525999,  0.26752999, -0.084809  ,  0.29764   ,\n",
       "        0.062339  , -0.15475   ,  0.17783999,  0.32328001, -0.92751998,\n",
       "        0.15194   ,  0.16324   , -0.10428   , -0.026464  ,  0.65970999,\n",
       "        0.14782   ,  0.38622999,  0.25169   ,  0.1261    , -0.43138   ,\n",
       "        0.28092   ,  3.16039991, -0.17565   , -0.0032247 ,  0.64389002,\n",
       "       -0.39697   ,  0.18975   ,  0.37999001, -0.079175  , -0.14781   ,\n",
       "       -0.072965  ,  0.057247  , -0.42313999,  0.4508    , -0.097386  ,\n",
       "       -0.47587001, -0.96599001, -0.75594997, -0.033932  , -0.070886  ,\n",
       "       -0.44828001, -0.52094001, -0.1823    ,  0.18582   , -0.074273  ,\n",
       "       -0.017871  ,  0.16742   ,  0.015459  ,  0.30289999, -0.1258    ,\n",
       "        0.32418001, -0.31263   , -0.076832  ,  0.051959  ,  0.27241999,\n",
       "       -0.18285   , -0.36478999, -0.63562   , -0.21685   ,  0.035812  ,\n",
       "        0.12485   ,  0.37268001, -0.16976   , -0.094146  , -0.16412   ,\n",
       "       -0.10728   ,  0.037866  ,  0.1175    , -0.15533   ,  0.34062001,\n",
       "        0.58848   ,  0.38992   , -0.54838997,  0.85013002, -0.83727998,\n",
       "        0.15482   , -0.37191001, -0.65408999, -0.27631   , -0.025224  ,\n",
       "        0.075732  , -0.23904   , -0.18311   , -0.084571  ,  0.15492   ,\n",
       "       -0.16316999, -0.26499   ,  0.056831  ,  0.88287002, -0.47655001,\n",
       "        0.25130999, -0.09316   ,  0.34377   , -0.35863   , -0.22855   ,\n",
       "        0.11918   ,  0.29661   , -0.2536    ,  0.049002  , -0.21234   ,\n",
       "        0.16237   ,  0.53871   ,  0.035344  ,  0.39293   , -0.29673001,\n",
       "       -0.72556001, -0.27430999,  1.34689999, -0.19217999,  0.50533998,\n",
       "        0.028451  , -0.32205999,  0.096035  , -0.0083551 , -0.013107  ,\n",
       "       -0.32444   , -0.10163   ,  0.031755  , -0.63195997, -0.21540999,\n",
       "       -0.035609  ,  0.31259   ,  0.23988   , -0.19056   , -0.13086   ,\n",
       "       -0.12644   ,  0.48795   , -0.13492   , -0.41966999,  0.15904   ,\n",
       "       -0.27921   , -0.017258  ,  0.29370001,  0.067436  ,  0.085052  ,\n",
       "        0.099394  , -0.0055281 ,  0.094985  ,  0.11167   ,  0.19749001,\n",
       "        0.25229999,  0.32205001,  0.42778   , -0.03518   ,  1.32910001,\n",
       "        0.005261  ,  0.26769   , -0.46168   ,  0.1125    ,  0.10111   ,\n",
       "       -0.31174001,  0.54579997, -0.37362999, -0.026133  ,  0.99566001,\n",
       "       -0.15827   , -0.26201999,  0.17324001,  0.060104  , -0.48004001,\n",
       "        0.23841   , -0.21495   ,  0.077693  , -0.089078  ,  0.12985   ,\n",
       "       -0.17399999, -0.057151  ,  0.48207   , -0.14668   ,  0.26739001,\n",
       "       -0.33366001,  0.32552001,  0.62519997, -0.30904999,  0.087737  ,\n",
       "       -0.17204   ,  0.28246   , -0.037268  ,  0.16007   ,  0.30030999,\n",
       "        1.40610003, -0.32168999, -0.025792  ,  0.037175  ,  0.026222  ,\n",
       "       -0.27671   ,  0.051688  , -0.058734  , -0.23222999, -0.10529   ,\n",
       "       -0.40318   , -0.22160999,  0.060587  ,  0.091321  , -0.21363001,\n",
       "        0.071634  , -0.21331   ,  0.074621  ,  0.012001  , -0.21952   ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three words in the vocab dictionary are (('<oov>', 1), ('startofseq', 2), ('endofseq', 3))\n",
    "\n",
    "emb_matrix[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test = 30\n",
    "training = 70\n",
    "\n",
    "key_list = list(img_cap_dic.keys())   \n",
    "\n",
    "test_key_count = int((len(key_list) / 100) * test) \n",
    "test_keys = [random.choice(key_list) for ele in range(test_key_count)] \n",
    "train_keys = [ele for ele in key_list if ele not in test_keys] \n",
    " \n",
    "testing_dict = dict((key, img_cap_dic[key]) for key in test_keys  \n",
    "                                        if key in img_cap_dic) \n",
    "\n",
    "training_dict = dict((key, img_cap_dic[key]) for key in train_keys  \n",
    "                                        if key in img_cap_dic) \n",
    "\n",
    "  \n",
    "testing_features = dict((key, features[key]) for key in test_keys  \n",
    "                                        if key in features)  \n",
    "training_features = dict((key, features[key]) for key in train_keys  \n",
    "                                        if key in features) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 31)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 31, 256)      2129152     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 31, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8317)         2137469     dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,906,557\n",
      "Trainable params: 5,906,557\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "number_pics_per_bath = 3\n",
    "steps = len(img_cap_dic)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2697 steps\n",
      "2697/2697 [==============================] - 215s 80ms/step - loss: 4.4965 - accuracy: 0.2283\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2697 steps\n",
      "2697/2697 [==============================] - 232s 86ms/step - loss: 3.6693 - accuracy: 0.2880\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2697 steps\n",
      "2697/2697 [==============================] - 243s 90ms/step - loss: 3.3940 - accuracy: 0.3072\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2697 steps\n",
      "2697/2697 [==============================] - 267s 99ms/step - loss: 3.2203 - accuracy: 0.3192\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2697 steps\n",
      "2697/2697 [==============================] - 257s 95ms/step - loss: 3.0965 - accuracy: 0.3282\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(img_cap_dic, features, tokenizer, max_length, number_pics_per_bath)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"image_captioning_model\" + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b0a44e9eb9e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# model = load_model('../input/image-captioning-models/final.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mvgmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m#vgmodel=VGG16()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vgg16' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_features(filename, model):\n",
    "        try:\n",
    "            image = Image.open(filename)\n",
    "        except:\n",
    "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image)\n",
    "        return feature\n",
    "    \n",
    "    \n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'start_'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        \n",
    "        pred = model.predict([photo,sequence], verbose=0)\n",
    "        pred = np.argmax(pred)\n",
    "                \n",
    "        word = word_for_id(pred, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if (word == '_end'):\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "def remove_repeated_words(text):\n",
    "   \n",
    "    tokens = text.split(' ')\n",
    "    for i in range(1, len(tokens)):\n",
    "        tokens1 = text.split(' ')\n",
    "        if tokens1[len(tokens1)-1]==tokens1[len(tokens1)-2]:\n",
    "            tokens1.pop(len(tokens1)-1)\n",
    "        text = (\" \".join(tokens1))\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "max_length=31\n",
    "img_path = \"../input/myimages/1001773457_577c3a7d70.jpg\"\n",
    "\n",
    "vgmodel=VGG16()\n",
    "vgmodel.layers.pop()\n",
    "vgmodel = Model(inputs=vgmodel.inputs, outputs=vgmodel.layers[-2].output)\n",
    "\n",
    "photo = extract_features(img_path, vgmodel)\n",
    "img = Image.open(img_path)\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "description = remove_repeated_words(description)\n",
    "\n",
    "print(description.replace('start_', '').replace('_end', '').replace('end', ''))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
